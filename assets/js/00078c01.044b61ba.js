"use strict";(self.webpackChunkmy_docu_website=self.webpackChunkmy_docu_website||[]).push([[3890],{9611:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var a=n(4848),i=n(8453);const s={},o="Dataset curation",r={id:"machine-learning/datasets/DatasetCuration",title:"Dataset curation",description:"Machine learning models learn through the presentation of examples and the optimization toward the loss defined. A good model is a model that can generalize well for the given task and to achieve this behavior we must make the model internalize the dataset variability. The model internalizes the data variability according the number and the quality of the presented dataset, so to have good models fundamentally we need a good dataset.",source:"@site/docs/machine-learning/datasets/202103242007-DatasetCuration.md",sourceDirName:"machine-learning/datasets",slug:"/machine-learning/datasets/DatasetCuration",permalink:"/docs/machine-learning/datasets/DatasetCuration",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/machine-learning/datasets/202103242007-DatasetCuration.md",tags:[],version:"current",sidebarPosition:202103242007,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Dataset and AI bias",permalink:"/docs/machine-learning/datasets/AiBias"},next:{title:"Data exploration",permalink:"/docs/machine-learning/datasets/DatasetExploration"}},l={},d=[{value:"A good dataset",id:"a-good-dataset",level:2},{value:"Dataset collection",id:"dataset-collection",level:2},{value:"Data Curation",id:"data-curation",level:2},{value:"Important questions to get the loop working well",id:"important-questions-to-get-the-loop-working-well",level:3},{value:"List of triggers where we can find good samples to use",id:"list-of-triggers-where-we-can-find-good-samples-to-use",level:3},{value:"Self supervised learning",id:"self-supervised-learning",level:2}];function h(e){const t={a:"a",blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"dataset-curation",children:"Dataset curation"})}),"\n",(0,a.jsx)(t.p,{children:"Machine learning models learn through the presentation of examples and the optimization toward the loss defined. A good model is a model that can generalize well for the given task and to achieve this behavior we must make the model internalize the dataset variability. The model internalizes the data variability according the number and the quality of the presented dataset, so to have good models fundamentally we need a good dataset."}),"\n",(0,a.jsx)(t.p,{children:"Interestingly Andrew NG cites that we have two mindsets that we can work:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["The model centric","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"hold the data fixed and improve the code/the model."}),"\n",(0,a.jsx)(t.li,{children:"when we find problems we ask: How do I change the model?"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["The data centric","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"hold the code fixed and iteratively improve the data"}),"\n",(0,a.jsx)(t.li,{children:"when we find problems we ask: How do I improve the data?"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"a-good-dataset",children:"A good dataset"}),"\n",(0,a.jsx)(t.p,{children:"According with Andrew NG. we can define a good dataset when:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"The labels are consistent (Same label situations, labeled equally);"}),"\n",(0,a.jsx)(t.li,{children:"The variability of scenes and objects cover all important cases;"}),"\n",(0,a.jsx)(t.li,{children:"The dataset is updated constantly from the production data to cover data drift;"}),"\n",(0,a.jsx)(t.li,{children:"The size is proper for the given task."}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"dataset-collection",children:"Dataset collection"}),"\n",(0,a.jsx)(t.p,{children:"To acquire a good dataset, first we need to pass through the definition of the task:"}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsx)(t.p,{children:"ML lifecycle:"}),"\n",(0,a.jsx)(t.p,{children:"Scope the project -> Define/Collect data -> Training the model -> Deploy the model in production"}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["After we have defined the task we should make a manual with ",(0,a.jsx)(t.em,{children:"labeling instructions"}),", the idea here is that we formalize how the labeling will occur so we can better handle ambiguous cases."]}),"\n",(0,a.jsx)(t.p,{children:"Next we collect the data from different scenarios so that we get a good coverage of where we are going to use the model. The data collected should resemble the variance in the generator that generates images for the model."}),"\n",(0,a.jsx)(t.p,{children:"It seems that a common starting size to work with deep learning models is from 1k samples to 10k samples, of course enlarging this dataset normally is beneficial to the task."}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsx)(t.p,{children:"When the dataset is small label consistency is important."}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"data-curation",children:"Data Curation"}),"\n",(0,a.jsx)(t.p,{children:"After you have a dataset, we may find some problems and you need to improve it gradually to the task that you are working with. We can follow this guidelines:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Find a lot examples that the network doesn't do what you need it to do, then label this situations with the correct label."}),"\n",(0,a.jsxs)(t.li,{children:["After that you need to put this examples in the training set, meaning the new training set will be the old dataset + the new samples (Note: You can also remove ",(0,a.jsx)(t.em,{children:"boring images"})," so the learning can go faster)"]}),"\n",(0,a.jsx)(t.li,{children:"Then we restart the process, the faster we can spin this loop, the faster we will get better models"}),"\n"]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsx)(t.p,{children:"Note: Andrej Karpathy notes that we are on a nice position right now, if we enlarge the processing power and enlarge the data we can keep improving the deep learning model."}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"important-questions-to-get-the-loop-working-well",children:"Important questions to get the loop working well"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"How do I define and collect my data;"}),"\n",(0,a.jsx)(t.li,{children:"How do I modify my data to improve performance;"}),"\n",(0,a.jsx)(t.li,{children:"What metrics do I need to track concept/data drift;"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"list-of-triggers-where-we-can-find-good-samples-to-use",children:"List of triggers where we can find good samples to use"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Detection flickering"}),"\n",(0,a.jsx)(t.li,{children:"User warning (Normally is caused by a bad prediction)"}),"\n",(0,a.jsx)(t.li,{children:"Label oscillation"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"self-supervised-learning",children:"Self supervised learning"}),"\n",(0,a.jsx)(t.p,{children:"When we are constrained into the resources and time that we can use to label images, we can think about using self supervised learning. It is a technique that we can profit of data similarities to learn latent information about the task at hand, not needing to create manual labels."}),"\n",(0,a.jsx)(t.h1,{id:"links",children:"Links"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://shows.acast.com/the-robot-brains/episodes/andrej-karpathy-on-the-visionary-ai-in-teslas-autonomous-dri",children:"The Robot Brains Podcast - Andrej Karpathy on the visionary AI in Tesla's autonomous driving"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=06-AZXmwHjo",children:"A Chat with Andrew on MLOps: From Model-centric to Data-centric AI"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"/docs/machine-learning/datasets/AiBias",children:"Dataset Bias"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.aquariumlearning.com",children:"Aquarium Learning"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://labelerrors.com/",children:"Label Errors in ML Test Sets"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.jeremyjordan.me/testing-ml/",children:"Effective testing for machine learning systems"})}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var a=n(6540);const i={},s=a.createContext(i);function o(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);